{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be7f53f-479f-42b7-a2da-41adc50482dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf130885-e687-4f20-91b3-262b6570d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from scipy.stats import skew, iqr\n",
    "\n",
    "#col_idx_dict = {housing.columns[i]:i for i in range(housing.shape[1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061bbd2-9b56-4bdc-ad94-3f7d8d16377e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# AddFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2408ba13-d002-4d43-8bd8-3c052f2c1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddFeatures(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Adds new features to the dataset.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): (m, n) Data points\n",
    "        \n",
    "    Returns:\n",
    "        X (np.ndarray): (m, n + 6) Dataset with added features\n",
    "    '''\n",
    "    def fit(self, X):\n",
    "        \n",
    "        # return self\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        # create arrays of new features\n",
    "        rooms_per_household      = X[:, col_idx_dict['total_rooms']]    / X[:, col_idx_dict['households']]\n",
    "        income_per_household     = X[:, col_idx_dict['median_income']]  / X[:, col_idx_dict['households']] * X[:, col_idx_dict['population']]\n",
    "        income_per_population    = X[:, col_idx_dict['median_income']]  / X[:, col_idx_dict['population']] * X[:, col_idx_dict['households']]\n",
    "        bedrooms_per_rooms       = X[:, col_idx_dict['total_bedrooms']] / X[:, col_idx_dict['total_rooms']]\n",
    "        population_per_household = X[:, col_idx_dict['population']]     / X[:, col_idx_dict['households']]\n",
    "        rooms_per_age            = X[:, col_idx_dict['total_rooms']]    / X[:, col_idx_dict['housing_median_age']]\n",
    "        \n",
    "        # return concatenated dataset\n",
    "        return np.c_[X, rooms_per_household, income_per_household, income_per_population, bedrooms_per_rooms, population_per_household, rooms_per_age]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e3717-c9cd-4bd5-ad76-208af35a550f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# DataDropper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e61beeb-8727-48fb-b504-3a33f1488ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDropper(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    This class drops points considered outliers given the feature and method of dropping them. \n",
    "    Available methods are:\n",
    "    \"fixed\"     - drops data that are <= choosen fixed threshold,\n",
    "    \"flexible\"  - drops data that are <= choosen quantile,\n",
    "    \"optimized\" - drops data with the help of a simple loss function:\n",
    "                  $L(i, p) = S(i) + i^{p}$\n",
    "                  where, i is the ith dropped data point, S(i) is the skewness after dropping\n",
    "                  the ith data point and p is the penalty. The main assumption is that with\n",
    "                  highly skewed features dropping points will monotonically lower the skewness.\n",
    "                  Adding a penalty of $i^{p}$ creates a global minimum, that we seek to find,\n",
    "    \"skewness\"  - drops data untill some fixed value of skewness is satisfied.\n",
    "    \n",
    "    Args:\n",
    "        method (str)                     - method to use when dropping the data\n",
    "        feature (str)                    - feature variable to drop data from\n",
    "        val (float | int)                - value for \"fixed\", \"flexible\", and \"skewness\" methods.\n",
    "                                           For \"fixed\" the value must be some point in the interval of given\n",
    "                                           feature. For \"flexible\" method the value must be a quantile [0, 1] \n",
    "                                           which will specify the threshold for dropping points. For \"skewness\"\n",
    "                                           the value must be a minimal skewness we want to obtain, it must be\n",
    "                                           lower than the skewness of the feature itself.\n",
    "        penalty (str, default \"sq_root\") - specifies what penalty is added to the loss after dropping ith point.\n",
    "                                           If \"linear\" we take i to the 1th power. If \"sq_root\" i is raised to\n",
    "                                           the power of 0.5.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, method, feature, penalty=\"sq_root\", val=None):\n",
    "        assert(method in ['fixed', 'flexible', 'optimized', 'skewness']), 'methods available: \"fixed\", \"flexible\", \"optimized\", \"skewness\".'\n",
    "        assert(feature in col_idx_dict.keys()), f'feature must be one of : {list(col_idx_dict.keys())}'\n",
    "        self.method = method\n",
    "        self.val = val\n",
    "        self.feature = feature\n",
    "        self.col_index = col_idx_dict[self.feature]\n",
    "        self.penalty = penalty\n",
    "        \n",
    "    def check_val(self, val):\n",
    "        assert((type(val) == float) or (type(val) == int)), f'specify the value (int, float) for right parameter of {self.method} method.'\n",
    "        \n",
    "    def flag_outliers(self, X):\n",
    "        _iqr = iqr(X[:, self.col_index])\n",
    "        _lower_bound = np.quantile(X[:, self.col_index], 0.25) - 1.5 * _iqr\n",
    "        _upper_bound = np.quantile(X[:, self.col_index], 0.75) + 1.5 * _iqr\n",
    "        return X[(X[:, self.col_index] <= _lower_bound) | (X[:, self.col_index] >= _upper_bound), self.col_index]\n",
    "    \n",
    "    def fit(self, X):\n",
    "        if self.method == 'fixed':\n",
    "            # make sure that fix value is specified\n",
    "            self.check_val(self.val)\n",
    "            # make sure that the point is in the interval of data\n",
    "            _max_val = X[:, self.col_index].max()\n",
    "            _min_val = X[:, self.col_index].min()\n",
    "            assert((self.val <= _max_val) and (self.val >= _min_val)), 'the value must be in interval [min, max].'\n",
    "            self.idx_to_keep = X[:, self.col_index] <= self.val\n",
    "            \n",
    "        if self.method == 'flexible':\n",
    "            # make sure that flex value is specified\n",
    "            self.check_val(self.val)\n",
    "            # make sure that the quantile is in [0, 1]\n",
    "            assert((self.val <= 1) and (self.val >= 0)), 'the value must be in interval [0, 1].'\n",
    "            self.idx_to_keep = X[:, self.col_index] <= np.quantile(X[:, self.col_index], self.val)\n",
    "            \n",
    "        if self.method == 'optimized':\n",
    "            \n",
    "            assert(self.penalty in ['linear', 'sq_root']), 'penalties available: \"linear\", \"sq_root\".'\n",
    "            if self.penalty == 'linear':\n",
    "                self.penalty = 1\n",
    "            else:\n",
    "                self.penalty = 0.5\n",
    "            \n",
    "            loss = np.zeros(1)\n",
    "            skewness_of_data = skew(X[:, self.col_index])\n",
    "            sorted_data = np.sort(self.flag_outliers(X))[::-1]\n",
    "            for point, data in enumerate(sorted_data):\n",
    "                if point < skewness_of_data:\n",
    "                    loss = np.c_[loss, skew(X[X[:, self.col_index] <= data, self.col_index]) + point ** self.penalty]\n",
    "                else:\n",
    "                    break\n",
    "            loss = loss[loss > 0].flatten()\n",
    "            self.optimization_history = loss\n",
    "            self.idx_to_keep = X[:, self.col_index] <= sorted_data[loss.argmin()]\n",
    "            \n",
    "        if self.method == 'skewness':\n",
    "            # make sure that skew value is specified\n",
    "            self.check_val(self.val)\n",
    "            # make sure that the skew value is lower than max skewness observed\n",
    "            max_skew = skew(X[:, self.col_index])\n",
    "            assert(self.val <= max_skew), f'Value for skewness is higher than the maximum skewness observed in the data: {self.val} > {round(max_skew, 1)}.'\n",
    "            sorted_data = np.sort(X[:, self.col_index])[::-1]\n",
    "            for point, data in enumerate(sorted_data):\n",
    "                skew_after_drop = skew(X[X[:, self.col_index] <= data, self.col_index])\n",
    "                if skew_after_drop <= self.val:\n",
    "                    self.idx_to_keep = X[:, self.col_index] <= data\n",
    "                    break\n",
    "        \n",
    "        self.number_deleted = X.shape[0] - self.idx_to_keep.sum()\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X[self.idx_to_keep, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78df37-4561-4532-997a-9b07d8099542",
   "metadata": {},
   "source": [
    "## multi_features_outliers_droppper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017f83b2-172f-47a4-ad8c-766d01cdac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_features_outliers_dropper(X, features, method, val=None, penalty=None):\n",
    "    \n",
    "    assert(len(set(features).intersection(col_idx_dict.keys())) == len(features)), f'features must be a list of features from the data: {list(col_idx_dict.keys())}'\n",
    "    \n",
    "    X = X.values\n",
    "    \n",
    "    for feature in features:\n",
    "        _outlier_remover = DataDropper(method=method, feature=feature, val=val, penalty=penalty)\n",
    "        _outlier_remover.fit(X)\n",
    "        X = _outlier_remover.transform(X)\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0d1cb-37dc-41d3-b83e-5e2d94a7c971",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# small_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "224c97aa-44c1-41f5-a628-40702780ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class small_PCA(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Finds principal components of centered (not standardized) data. It does not use SVD approach.\n",
    "    The method finds weights of orthogonal projections of data points onto a subspace that is \n",
    "    spanned by the basis constructed of orthonormal eigenvectors with highest eigenvalues. \n",
    "    Eigenvectors and eigenvalues are obtained by eigendecomposition of covariance matrix. The size\n",
    "    of the basis is specified by the value of n_components.\n",
    "    \n",
    "    Args:\n",
    "        n_components (int) - rank of reduced data\n",
    "        X (np.ndarray)     - (m, n) dataset\n",
    "        \n",
    "    Methods:\n",
    "        fit(X)       - estimates the orthonormal eigenbasis for the subspace\n",
    "        transform(X) - produces the weights of orthogonal projections\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_components=None):\n",
    "        '''\n",
    "        Params:\n",
    "            n_components (int) - rank of reduced data\n",
    "        '''\n",
    "        \n",
    "        # check if the number of components has the right type and is greater than 0\n",
    "        assert(isinstance(n_components, int) and (n_components > 0)), f'n_components must be an integer greater than 0.'\n",
    "        \n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def fit(self, X):\n",
    "        '''\n",
    "        The method finds eigendecomposition of centered data of form $Q \\Lambda Q^{T}$. Then,\n",
    "        n - n_components lowest eigenvalues and eigenvectors are dropped. The columns of resulting\n",
    "        basis span the subspace in question. Also, the values of eigenvalues are stored as well\n",
    "        as their ratio.\n",
    "        \n",
    "        Params:\n",
    "            X (np.ndarray) - (m, n) dataset\n",
    "        '''\n",
    "        \n",
    "        # check if number of components is lower than the number of columns of data\n",
    "        assert(self.n_components <= X.shape[1]), f'n_components are greater than number of features. {self.n_components} > {X.shape[1]}'\n",
    "        \n",
    "        # calculate and eigendecompose the covariance matrix\n",
    "        cov_matrix = np.cov((X - X.mean(axis=0)).T)\n",
    "        s, Q = np.linalg.eig(cov_matrix)\n",
    "        \n",
    "        # get n_components with highest values of eigenvalues\n",
    "        Q_r = Q[:, s.argsort()[-self.n_components:]]\n",
    "        self.reduced_basis = Q_r[:, ::-1] # ---> this assures the resulting data is similar with sklearn's PCA\n",
    "        self.explained_variance_ = s\n",
    "        self.explained_variance_ratio_ = s / s.sum()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        Reduce the dataset by finding the weights of orthogonal projections on the subspace Col(Q_r).\n",
    "        Since Q_r has orthonormal columns, the weights can be obtained from $(Q_{r}^{T}X^{T})^{T} = XQ_{r}$.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray)   - (m, n) dataset\n",
    "            \n",
    "        Returns:\n",
    "            X_r (np.ndarray) - (m, n_components) principal components\n",
    "        '''\n",
    "        # return mean 0 results (it is not necessary at all, but the results will be the same as with sklearn's PCA)\n",
    "        return (X - X.mean(axis=0)) @ self.reduced_basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e280cb2f-ce9b-415c-b15f-e5dc7ba015d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# MulticollinearityHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e99c50f9-3b55-446b-8035-1fa5045b68d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticollinearityHandler(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    This class tries to deal with multicollinearity problem implementing three different methods.\n",
    "    1. Simply drop all but one feature. This method leaves the one feature that has the highest\n",
    "       correlation with the target variable.\n",
    "    2. Use small_PCA to reduce the dimention of the correlated features within a specified group.\n",
    "    3. Uses method described by Min Tsao et al. (sorce at the end) It tries to find a vector of\n",
    "       weights w that finds the overall effect of the group.\n",
    "       \n",
    "    The methods described above can be choosen by specifying the method argument, respectively:\n",
    "    \"drop\"          - drops all but one feature within group\n",
    "    \"pca\"           - uses small_PCA class\n",
    "    \"gr_treatement\" - uses the method of M. Tsao\n",
    "       \n",
    "    Since two highly correlated groups were recognized, there is only option to choose one of the\n",
    "    groups:\n",
    "    \"first\"  - ['total_rooms', 'population', 'total_bedrooms', 'households']\n",
    "    \"second\" - ['income_per_household', 'median_income', 'income_per_population']\n",
    "    \n",
    "    Args:\n",
    "        method (str)       - method to choose when reducing the data\n",
    "        group (str)        - one of [\"first\", \"second\"]\n",
    "        n_components (int) - rank of reduced data when using pca method\n",
    "        X (np.ndarray)     - (m, n) dataset\n",
    "        \n",
    "    Methods:\n",
    "        get_indicies_for_group(group)\n",
    "            finds column numbers of features given the specified group. \n",
    "            \n",
    "        get_the_least_corr_features_group(X, gropu)\n",
    "            finds the features that has the least correlation with the target variable\n",
    "            \n",
    "        fit(X)\n",
    "            given the selected method it proceedes with finding the indexes that should be dropped\n",
    "            \n",
    "        transform(X)\n",
    "            transforms the data using the indexes found in fit() method\n",
    "            \n",
    "    Sorce:\n",
    "        Min Tsao \"Group least squares regression for linear models with strongly correlated predictor\n",
    "        variables\". \n",
    "        DOI:      https://doi.org/10.1007/s10463-022-00841-7\n",
    "        arXiv:    https://doi.org/10.48550/arXiv.1804.02499\n",
    "        Accessed: 2/20/23\n",
    "        \n",
    "    '''    \n",
    "    def __init__(self, method, group, n_components=None):\n",
    "        '''\n",
    "        Params:\n",
    "            method (str)       - method to choose when reducing the data\n",
    "            group (str)        - one of [\"first\", \"second\"]\n",
    "            n_components (int) - rank of reduced data when using \n",
    "        '''\n",
    "        assert(method in ['drop', 'pca', 'gr_teatement']), f'Method {method} is invalid. Available methods: [\"drop\", \"pca\", \"gr_treatement\"].'\n",
    "        assert(group in ['first', 'second']), f'Group {group} is invalid. Please, choose one of available: [\"first\", \"second\"]. For more information consult documentation.'\n",
    "        self.method = method\n",
    "        self.group = group\n",
    "        self.groups = dict(first = ['total_rooms', 'population', 'total_bedrooms', 'households'],\n",
    "                           second = ['income_per_household', 'median_income', 'income_per_population'])\n",
    "        self.n_components = n_components\n",
    "    \n",
    "    def get_indicies_for_group(self, group):\n",
    "        '''\n",
    "        Finds column numbers of features given the specified group. The method uses a globaly specified\n",
    "        dictionary that specifies the column number given the feature name. The dictionary must be \n",
    "        specified beforehand. The main pipeline is responsible for creating this dictionary.\n",
    "        \n",
    "        Args:\n",
    "            group (str) - one of [\"first\", \"second\"]\n",
    "        \n",
    "        Returns:\n",
    "            _indicies (list) - list of indicies that specifies the pair feature - column number\n",
    "        '''\n",
    "        \n",
    "        # create a matrix [feature_name index_number]\n",
    "        tmp_holder = np.array([*col_idx_dict.items()])\n",
    "        \n",
    "        #get the indexes for given group\n",
    "        _indicies = [\n",
    "            tmp_holder[tmp_holder[:, 0] == col, 1].astype(int)[0] \n",
    "            for col in self.groups[group]\n",
    "        ]\n",
    "        \n",
    "        return _indicies\n",
    "    \n",
    "    def get_the_least_corr_features_idx(self, X, group):\n",
    "        '''\n",
    "        Finds the features that has the least correlation with the target variable. The method uses a \n",
    "        globaly specified dictionary that specifies the column number given the feature name. The \n",
    "        dictionary must be specified beforehand. The main pipeline is responsible for creating this dictionary.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray) - (m, n) dataset\n",
    "            group (str)    - one of [\"first\", \"second\"]\n",
    "            \n",
    "        Returns:\n",
    "            _low_corr_features_idx (np.ndarray) - array of column numbers for the least correlated with\n",
    "                                                  the target variable features\n",
    "        '''\n",
    "        # get the group indexes\n",
    "        group_indicies = self.get_indicies_for_group(group)\n",
    "        # mask\n",
    "        X_indexed = X[:, group_indicies + [col_idx_dict['median_house_value']]]\n",
    "        # calculate correlation with the target variable\n",
    "        _correlation = np.corrcoef(X_indexed.T)[-1, :-1]\n",
    "        # get the indexes of the least correlated features\n",
    "        _low_corr_features_idx = np.array(group_indicies)[_correlation.argsort()[:-1]]\n",
    "        return _low_corr_features_idx\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \n",
    "        if self.method == 'drop':\n",
    "            self.idx_to_drop = self.get_the_least_corr_features_idx(X, self.group)\n",
    "        \n",
    "        if self.method == 'pca':\n",
    "            assert(self.n_components is not None), f'Specify the number of components.'\n",
    "            # get the group indexes\n",
    "            self.idx_to_reduce = self.get_indicies_for_group(self.group)\n",
    "            # pca should be done in transform\n",
    "            \n",
    "        if self.method == 'gr_treatement':\n",
    "            # to be added, i have to first figure out the context of the article.\n",
    "            pass\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        if self.method == 'drop':\n",
    "            return np.delete(X, self.idx_to_drop, axis=1)\n",
    "        \n",
    "        if self.method == 'pca':\n",
    "            s_pca = small_PCA(n_components=self.n_components)\n",
    "            X_reduced = s_pca.fit_transform(X[:, self.idx_to_reduce])\n",
    "            X_concat = np.c_[np.delete(X, self.idx_to_reduce, axis=1),\n",
    "                             X_reduced]\n",
    "            return X_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7850969d-dd89-4cbf-ac8d-8a274b6bc177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
